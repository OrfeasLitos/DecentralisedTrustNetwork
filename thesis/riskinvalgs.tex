\section{Risk Invariance Algorithms}
  \subimport{thesis/}{riskinvalgsintro.tex}
  \subimport{thesis/definitions/}{trustreduction.tex}

  \subimport{thesis/theorems/}{saturationtheorem.tex}
  \subimport{thesis/proofs/}{saturationproof.tex}

  \subimport{thesis/theorems/}{trusttransfertheorem.tex}
  \subimport{thesis/proofs/}{trusttransferproof.tex}

  \subimport{thesis/definitions/}{restrictedflow.tex}

  \subimport{thesis/lemmas/}{flowlimitlemma.tex}
  \subimport{thesis/proofs/}{flowlimitproof.tex}

  \subimport{thesis/theorems/}{trustsavingtheorem.tex}
  \subimport{thesis/proofs/}{trustsavingproof.tex}

  \subimport{thesis/theorems/}{invtrustrednaivetheorem.tex}
  \subimport{thesis/proofs/}{invtrustrednaiveproof.tex}

  Until now $MaxFlow$ has been viewed purely as an algorithm. This algorithm is not guaranteed to always return the same
  flow when executed muliple times on the same graph. However, the corresponding flow value, $maxFlow$, is always the same.
  Thus $maxFlow$ can be also viewed as a function from a matrix of capacities to a non-negative real number. Under this
  perspective, we prove the following theorem. Let $\mathcal{C}$ be the family of all capacity matrices
  $C = [c_{vw}]_{V\left(\mathcal{G}\right) \times V\left(\mathcal{G}\right)}$.
  \subimport{thesis/theorems/}{maxflowcontinuitytheorem.tex}
  \subimport{thesis/proofsketches/}{maxflowcontinuityproofsketch.tex}

  Here we show three naive algorithms for calculating new direct trusts so as to maintain invariable risk when paying
  a trusted party. Let $F = \sum\limits_{i=1}^{n}x_i$. To prove the correctness of the algorithms, it suffices to prove that
  \begin{equation}
  \label{naive:req1}
     \forall i \in [n], c'_i \leq x_i \mbox{ and}
  \end{equation}
  \begin{equation}
  \label{naive:req2}
     \sum\limits_{i=1}^{n}c'_i = F - V \enspace.
  \end{equation}
  Proofs of correctness and complexity can be found in the Appendix.
  \subimport{thesis/algorithms/}{fcfscode.tex}
  This algorithm simply chooses to nullify all outgoing trust to one player after another in the order they are given at the
  input, until the desired indirect trust is achieved. The complexity of this algorithm is $O\left(n\right)$.
  \newpage
  \subimport{thesis/algorithms/}{abscode.tex}

  This algorithm finds a \texttt{reduction} value such that the configuration
  $\forall i \in \left[n\right], c'_i = \max{\left(0, x_i - \mbox{\texttt{reduction}}\right)}$ achieves the desired flow.

  The function \texttt{preprocess(}$x_i$\texttt{)} returns a data structure \texttt{X} containing the set of flows
  $\left(x_i\right)$, such that the corresponding function \texttt{popMin(X)} is able to repeatedly return a tuple
  consisiting of the index of the minimum element and a new data structure missing exactly the minimum element.
  Examples of such pairs of functions are:
  \begin{equation*}
  \begin{gathered}
    \begin{cases}
      \texttt{preprocess = quickSort} \\
      \texttt{popMin = (}x_1\texttt{, X}\setminus x_1\texttt{)}
    \end{cases}
    \mbox{ and} \\
    \begin{cases}
      \texttt{preprocess = FibonacciHeap} \\
      \texttt{popMin = (find-min(X),delete-min(X))}
    \end{cases} \enspace.
  \end{gathered}
  \end{equation*}
  In the general case, the complexity of the algorithm \texttt{abs} is proven to be
  $O\left(preprocess\right) + O\left(n\right)O\left(popMin\right)$. In both specific cases, the complexity is
  $O\left(n\log{n}\right)$. This algorithm also minimizes the $||\Delta_i||_\infty$ norm for the specific set of old flows
  $x_i$ given as input. A proof of this fact can be found in the Appendix.

  \subimport{thesis/algorithms/}{propcode.tex}
  We can see that this algorithm is simpler than the previous two. It simply reduces all outgoing direct trust proportionally
  in such a way that the desired flow is agieved. The complexity of this algorithm is proven to be $O\left(n\right)$.

  Naive algorithms result in $c'_i \leq x_i$, thus according to the Invariability Theorem (\ref{invariability}),
  $||\delta_i||_1$ is invariable for any of the possible results $C'$ of these algorithms and the resulting norm is not
  necessarily the minimum. The following algorithms concentrate on finding a configuration $C'$ that achieves $F' = F - V$
  while minimizing two $\delta_i$ norms, $||\delta_i||_\infty$ and $||\delta_i||_1$. We start with the
  $||\delta_i||_\infty$ minimizer.

  \subimport{thesis/algorithms/}{dinfmincode.tex}
  Since trust should be considered as a continuous unit and binary search bisects the interval containing the solution
  on each recursive call, inclusion of the $\epsilon$-parameters in \texttt{BinSearch} is necessary for the algorithm to
  complete in a finite number of steps.
  \subimport{thesis/algorithms/}{dinfbinsearchcode.tex}
  Let $\delta \in \left[0, \max\limits_{1 \leq i \leq n}{\{c_i\}}\right]$. Furthermore, let $C'$ such that
  $\forall i \in \left[n\right], c'_i = \max{\left(0, c_i - \delta\right)}$. We define $maxFlow\left(\delta\right) =
  maxFlow\left(A, B, C'\right) = F'$ and $MaxFlow\left(\delta\right) = X'$. Conventions similar to $F$, $X$ and $C$ hold
  for $F'$, $X'$ and $\delta$ as far as subscripts are concerned.
  \subimport{thesis/lemmas/}{maxflowmonotonicitylemma.tex}
  \subimport{thesis/proofsketched/}{maxflowmonotonicityproofsketch.tex}
  From the previous lemma we deduce that, given a $V \in \left(0, F\right)$, if we determine a $\delta$ such that
  $maxFlow\left(\delta\right) = F - V$, this $\delta$ is unique. Furthermore, it is
  \begin{equation*}
    ||\delta_i||_\infty = \max\limits_{1 \leq i \leq n}{\delta_i} = \max\limits_{1 \leq i \leq n}{\left(c_i - c'_i\right)} =
    \max\limits_{1 \leq i \leq n}{\left(c_i - \max{\left(0, c_i - \delta\right)}\right)} = \delta \enspace.
  \end{equation*}
  It is proven that the two algorithms work as expected, that is an invocation of \texttt{dinfmin} with valid inputs returns
  a capacity $C'$ that yields the desired $maxFlow$ and minimizes $||\delta_i||_\infty$. The complexity of \texttt{BinSearch}
  is $O\left(\left(maxFlow + n\right)\log_2\left(\frac{top - bot}{\epsilon_1 + \epsilon_2}\right)\right)$ and the complexity
  of \texttt{dinfmin} is
  $O\left(\left(maxFlow + n\right)\log_2\left(\frac{\delta_{max}}{\epsilon_1 + \epsilon_2}\right)\right)$.

  We will now concentrate on finding a capacity configuration $C'$ such that $F' = F - V$ and that minimizes
  $\sum\limits_{i=1}^{n}\left(c_i-c'_i\right) = ||\delta_i||_1$ as well. We treat the flow problem as a linear programming
  problem. Next we see the formulation of the problem in this form, along with a breakdown of each relevant matrix and
  vector.
  \subimport{thesis/lp/}{lpprimal.tex}
  We would like to find a solution that, except for maximizing the flow, also minimizes $||\delta_i||_1$ at the same time.
  More precisely, we would like to optimize
  \begin{equation*}
    \min{\sum\limits_{v \in \mathcal{V}}\left(c_{Av} - c'_{Av}\right)}
  \end{equation*}
  as well.
  Since we wish to optimize with regards to two objective functions, we approach the problem as follows: Initially, we ignore
  the minimization and derive the dual of the previous problem with respect to the maximisation. We then substitute the two
  problems' optimisations with an additional constraint that equates the two objective functions. Due to the Strong Duality
  theorem of linear programming \cite{amo}, this equality can be achieved only by the common optimal solution of the
  two problems. Next we treat the combination of constraints and variables of the primal and the dual problem, along with the
  newly introduced constraint and the previously ignored $||\delta_i||_1$ minimisation as a new linear problem. For every
  $j \in \left[n\right]$, the solution to this problem contains a $c'_{1j}$. These capacities will comprise the new
  configuration that player $A$ requires.

  We will now describe the dual problem in detail.
  \subimport{thesis/lp/}{lpdual.tex}
  Everything is now in place to define the linear problem whose solution $C'$ yields $maxFlow\left(C'\right) = F - V$ and
  minimizes $||\delta_i||_\infty$. The constraints are (\ref{lp:primal:constraints}) and (\ref{lp:dual:constraints})
  supplemented with a constraint that equates the two problems' optimality functions:
  \begin{equation*}
    \sum\limits_{j \in \left[n\right]}f'_{1j} = \sum\limits_{j \in \left[n\right]}c_{1j}y_{c1j} +
    \sum\limits_{i \in \left[n\right]}\sum\limits_{j \in \left[n\right]}c_{ij}y_{fcij} + \left(F - V\right)y_F \enspace.
  \end{equation*}
  The desired optimisation is
  \begin{equation*}
    \min{\sum\limits_{j \in \left[n\right]}\left(c_{1j} - c'_{1j}\right)} \enspace.
  \end{equation*}
  The final linear program consists of $2n^2 + 2n - 1$ variables and $2n^2 + 2n$ constraints. There exist a variety of
  algorithms that solve linear programs, such as simplex and ellipsoid algorithm. The ellipsoid algorithm requires polynomial
  time in the worst-case scenario, but for practical purposes algorithms of the simplex category seem to exhibit better
  behavior \cite{it}.
